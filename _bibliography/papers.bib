---
---

@article{srivastava2023offense,
      title={No Offense Taken: Eliciting Offensiveness from Language Models}, 
      author={Anugya Srivastava and Rahul Ahuja and Rohith Mukku},
      pdf = {https://arxiv.org/pdf/2310.00892.pdf},
      abstract = {This work was completed in May 2022. For safe and reliable deployment of language models in the real world, testing needs to be robust. This robustness can be characterized by the difficulty and diversity of the test cases we evaluate these models on. Limitations in human-in-the-loop test case generation has prompted an advent of automated test case generation approaches. In particular, we focus on Red Teaming Language Models with Language Models by Perez et al.(2022). Our contributions include developing a pipeline for automated test case generation via red teaming that leverages publicly available smaller language models (LMs), experimenting with different target LMs and red classifiers, and generating a corpus of test cases that can help in eliciting offensive responses from widely deployed LMs and identifying their failure modes.},
      year={2023},
      eprint={2310.00892},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      selected={true}
}

@article{srivastava2022distribution,
      title={Out of Distribution Detection on ImageNet-O}, 
      author={Anugya Srivastava and Shriya Jain and Mugdha Thigle},
      year={2022},
      pdf = {https://arxiv.org/pdf/2201.09352.pdf},
      abstract = {Out of distribution (OOD) detection is a crucial part of making machine learning systems robust. The ImageNet-O dataset is an important tool in testing the robustness of ImageNet trained deep neural networks that are widely used across a variety of systems and applications. We aim to perform a comparative analysis of OOD detection methods on ImageNet-O, a first of its kind dataset with a label distribution different than that of ImageNet, that has been created to aid research in OOD detection for ImageNet models. As this dataset is fairly new, we aim to provide a comprehensive benchmarking of some of the current state of the art OOD detection methods on this novel dataset. This benchmarking covers a variety of model architectures, settings where we haves prior access to the OOD data versus when we don't, predictive score based approaches, deep generative approaches to OOD detection, and more.},
      eprint={2201.09352},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      selected={true}
}
